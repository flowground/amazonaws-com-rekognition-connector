{
    "title": "Amazon Rekognition",
    "description": "This is the Amazon Rekognition API reference.",
    "docsUrl": "https://docs.aws.amazon.com/rekognition/",
    "url": "https://api.apis.guru/v2/specs/amazonaws.com/rekognition/2016-06-27/swagger.json",
    "envVars": {},
    "credentials": {
        "fields": {
            "server": {
                "label": "Server",
                "viewClass": "SelectView",
                "model": [
                    "https://rekognition.amazonaws.com/",
                    "http://rekognition.amazonaws.com/",
                    "--- Custom URL"
                ],
                "required": true
            },
            "otherServer": {
                "label": "Custom Server URL",
                "viewClass": "TextFieldView"
            },
            "auth_hmac": {
                "label": "Authorization (hmac)",
                "viewClass": "TextFieldView",
                "note": "Amazon Signature authorization v4"
            }
        }
    },
    "triggers": {
        "startFlow": {
            "main": "./lib/triggers/startFlow.js",
            "type": "polling",
            "title": "Start Flow",
            "fields": {}
        }
    },
    "actions": {
        "CompareFaces": {
            "main": "./lib/actions/CompareFaces.js",
            "title": "CompareFaces",
            "description": "Compares a face in the source input image with each of the 100 largest faces\ndetected in the target input image. \n\n If the source image contains multiple faces, the service detects the largest\nface and compares it with each face detected in the target image. \n\nYou pass the input and target images either as base64-encoded image bytes or as\nreferences to images in an Amazon S3 bucket. If you use the AWS CLI to call\nAmazon Rekognition operations, passing image bytes isn't supported. The image\nmust be formatted as a PNG or JPEG file. \n\nIn response, the operation returns an array of face matches ordered by\nsimilarity score in descending order. For each face match, the response provides\na bounding box of the face, facial landmarks, pose details (pitch, role, and\nyaw), quality (brightness and sharpness), and confidence value (indicating the\nlevel of confidence that the bounding box contains a face). The response also\nprovides a similarity score, which indicates how closely the faces match. \n\nBy default, only faces with a similarity score of greater than or equal to 80%\nare returned in the response. You can change this value by specifying the \nSimilarityThreshold parameter.\n\n CompareFaces also returns an array of faces that don't match the source image.\nFor each face, it returns a bounding box, confidence value, landmarks, pose\ndetails, and quality. The response also returns information about the face in\nthe source image, including the bounding box of the face and confidence value.\n\nIf the image doesn't contain Exif metadata, CompareFaces returns orientation\ninformation for the source and target images. Use these values to display the\nimages with the correct image orientation.\n\nIf no faces are detected in the source or target images, CompareFaces returns an \nInvalidParameterException error. \n\n This is a stateless API operation. That is, data returned by this operation\ndoesn't persist.\n\nFor an example, see Comparing Faces in Images in the Amazon Rekognition\nDeveloper Guide.\n\nThis operation requires permissions to perform the rekognition:CompareFaces \naction.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/CompareFaces.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "CreateCollection": {
            "main": "./lib/actions/CreateCollection.js",
            "title": "CreateCollection",
            "description": "Creates a collection in an AWS Region. You can add faces to the collection using\nthe IndexFaces operation. \n\nFor example, you might create collections, one for each of your application\nusers. A user can then index faces using the IndexFaces operation and persist\nresults in a specific collection. Then, a user can search the collection for\nfaces in the user-specific container. \n\nWhen you create a collection, it is associated with the latest version of the\nface model version.\n\nCollection names are case-sensitive.\n\nThis operation requires permissions to perform the rekognition:CreateCollection \naction.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/CreateCollection.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "CreateStreamProcessor": {
            "main": "./lib/actions/CreateStreamProcessor.js",
            "title": "CreateStreamProcessor",
            "description": "Creates an Amazon Rekognition stream processor that you can use to detect and\nrecognize faces in a streaming video.\n\nAmazon Rekognition Video is a consumer of live video from Amazon Kinesis Video\nStreams. Amazon Rekognition Video sends analysis results to Amazon Kinesis Data\nStreams.\n\nYou provide as input a Kinesis video stream (Input) and a Kinesis data stream (\nOutput) stream. You also specify the face recognition criteria in Settings. For\nexample, the collection containing faces that you want to recognize. Use Name to\nassign an identifier for the stream processor. You use Name to manage the stream\nprocessor. For example, you can start processing the source video by calling \nStartStreamProcessor with the Name field. \n\nAfter you have finished analyzing a streaming video, use StopStreamProcessor to\nstop processing. You can delete the stream processor by calling \nDeleteStreamProcessor.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/CreateStreamProcessor.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "DeleteCollection": {
            "main": "./lib/actions/DeleteCollection.js",
            "title": "DeleteCollection",
            "description": "Deletes the specified collection. Note that this operation removes all faces in\nthe collection. For an example, see delete-collection-procedure.\n\nThis operation requires permissions to perform the rekognition:DeleteCollection \naction.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/DeleteCollection.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "DeleteFaces": {
            "main": "./lib/actions/DeleteFaces.js",
            "title": "DeleteFaces",
            "description": "Deletes faces from a collection. You specify a collection ID and an array of\nface IDs to remove from the collection.\n\nThis operation requires permissions to perform the rekognition:DeleteFaces \naction.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/DeleteFaces.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "DeleteStreamProcessor": {
            "main": "./lib/actions/DeleteStreamProcessor.js",
            "title": "DeleteStreamProcessor",
            "description": "Deletes the stream processor identified by <code>Name</code>. You assign the value for <code>Name</code> when you create the stream processor with <a>CreateStreamProcessor</a>. You might not be able to use the same name for a stream processor for a few seconds after calling <code>DeleteStreamProcessor</code>.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/DeleteStreamProcessor.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "DescribeCollection": {
            "main": "./lib/actions/DescribeCollection.js",
            "title": "DescribeCollection",
            "description": "Describes the specified collection. You can use DescribeCollection to get\ninformation, such as the number of faces indexed into a collection and the\nversion of the model used by the collection for face detection.\n\nFor more information, see Describing a Collection in the Amazon Rekognition\nDeveloper Guide.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/DescribeCollection.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "DescribeStreamProcessor": {
            "main": "./lib/actions/DescribeStreamProcessor.js",
            "title": "DescribeStreamProcessor",
            "description": "Provides information about a stream processor created by <a>CreateStreamProcessor</a>. You can get information about the input and output streams, the input parameters for the face recognition being performed, and the current status of the stream processor.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/DescribeStreamProcessor.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "DetectFaces": {
            "main": "./lib/actions/DetectFaces.js",
            "title": "DetectFaces",
            "description": "Detects faces within an image that is provided as input.\n\n DetectFaces detects the 100 largest faces in the image. For each face detected,\nthe operation returns face details. These details include a bounding box of the\nface, a confidence value (that the bounding box contains a face), and a fixed\nset of attributes such as facial landmarks (for example, coordinates of eye and\nmouth), gender, presence of beard, sunglasses, and so on. \n\nThe face-detection algorithm is most effective on frontal faces. For non-frontal\nor obscured faces, the algorithm might not detect the faces or might detect\nfaces with lower confidence. \n\nYou pass the input image either as base64-encoded image bytes or as a reference\nto an image in an Amazon S3 bucket. If you use the to call Amazon Rekognition\noperations, passing image bytes is not supported. The image must be either a PNG\nor JPEG formatted file. \n\nThis is a stateless API operation. That is, the operation does not persist any\ndata.\n\nThis operation requires permissions to perform the rekognition:DetectFaces \naction.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/DetectFaces.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "DetectLabels": {
            "main": "./lib/actions/DetectLabels.js",
            "title": "DetectLabels",
            "description": "Detects instances of real-world entities within an image (JPEG or PNG) provided\nas input. This includes objects like flower, tree, and table; events like\nwedding, graduation, and birthday party; and concepts like landscape, evening,\nand nature. \n\nFor an example, see Analyzing Images Stored in an Amazon S3 Bucket in the Amazon\nRekognition Developer Guide.\n\n DetectLabels does not support the detection of activities. However, activity\ndetection is supported for label detection in videos. For more information, see\nStartLabelDetection in the Amazon Rekognition Developer Guide.\n\nYou pass the input image as base64-encoded image bytes or as a reference to an\nimage in an Amazon S3 bucket. If you use the AWS CLI to call Amazon Rekognition\noperations, passing image bytes is not supported. The image must be either a PNG\nor JPEG formatted file. \n\n For each object, scene, and concept the API returns one or more labels. Each\nlabel provides the object name, and the level of confidence that the image\ncontains the object. For example, suppose the input image has a lighthouse, the\nsea, and a rock. The response includes all three labels, one for each object. \n\n {Name: lighthouse, Confidence: 98.4629} \n\n {Name: rock,Confidence: 79.2097} \n\n {Name: sea,Confidence: 75.061} \n\nIn the preceding example, the operation returns one label for each of the three\nobjects. The operation can also return multiple labels for the same object in\nthe image. For example, if the input image shows a flower (for example, a\ntulip), the operation might return the following three labels. \n\n {Name: flower,Confidence: 99.0562} \n\n {Name: plant,Confidence: 99.0562} \n\n {Name: tulip,Confidence: 99.0562} \n\nIn this example, the detection algorithm more precisely identifies the flower as\na tulip.\n\nIn response, the API returns an array of labels. In addition, the response also\nincludes the orientation correction. Optionally, you can specify MinConfidence \nto control the confidence threshold for the labels returned. The default is 55%.\nYou can also add the MaxLabels parameter to limit the number of labels returned. \n\nIf the object detected is a person, the operation doesn't provide the same\nfacial details that the DetectFaces operation provides.\n\n DetectLabels returns bounding boxes for instances of common object labels in an\narray of Instance objects. An Instance object contains a BoundingBox object, for\nthe location of the label on the image. It also includes the confidence by which\nthe bounding box was detected.\n\n DetectLabels also returns a hierarchical taxonomy of detected labels. For\nexample, a detected car might be assigned the label car. The label car has two\nparent labels: Vehicle (its parent) and Transportation (its grandparent). The\nresponse returns the entire list of ancestors for a label. Each ancestor is a\nunique label in the response. In the previous example, Car, Vehicle, and \nTransportation are returned as unique labels in the response. \n\nThis is a stateless API operation. That is, the operation does not persist any\ndata.\n\nThis operation requires permissions to perform the rekognition:DetectLabels \naction.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/DetectLabels.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "DetectModerationLabels": {
            "main": "./lib/actions/DetectModerationLabels.js",
            "title": "DetectModerationLabels",
            "description": "Detects explicit or suggestive adult content in a specified JPEG or PNG format\nimage. Use DetectModerationLabels to moderate images depending on your\nrequirements. For example, you might want to filter images that contain nudity,\nbut not images containing suggestive content.\n\nTo filter images, use the labels returned by DetectModerationLabels to determine\nwhich types of content are appropriate.\n\nFor information about moderation labels, see Detecting Unsafe Content in the\nAmazon Rekognition Developer Guide.\n\nYou pass the input image either as base64-encoded image bytes or as a reference\nto an image in an Amazon S3 bucket. If you use the AWS CLI to call Amazon\nRekognition operations, passing image bytes is not supported. The image must be\neither a PNG or JPEG formatted file.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/DetectModerationLabels.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "DetectText": {
            "main": "./lib/actions/DetectText.js",
            "title": "DetectText",
            "description": "Detects text in the input image and converts it into machine-readable text.\n\nPass the input image as base64-encoded image bytes or as a reference to an image\nin an Amazon S3 bucket. If you use the AWS CLI to call Amazon Rekognition\noperations, you must pass it as a reference to an image in an Amazon S3 bucket.\nFor the AWS CLI, passing image bytes is not supported. The image must be either\na .png or .jpeg formatted file. \n\nThe DetectText operation returns text in an array of TextDetection elements, \nTextDetections. Each TextDetection element provides information about a single\nword or line of text that was detected in the image. \n\nA word is one or more ISO basic latin script characters that are not separated\nby spaces. DetectText can detect up to 50 words in an image.\n\nA line is a string of equally spaced words. A line isn't necessarily a complete\nsentence. For example, a driver's license number is detected as a line. A line\nends when there is no aligned text after it. Also, a line ends when there is a\nlarge gap between words, relative to the length of the words. This means,\ndepending on the gap between words, Amazon Rekognition may detect multiple lines\nin text aligned in the same direction. Periods don't represent the end of a\nline. If a sentence spans multiple lines, the DetectText operation returns\nmultiple lines.\n\nTo determine whether a TextDetection element is a line of text or a word, use\nthe TextDetection object Type field. \n\nTo be detected, text must be within +/- 90 degrees orientation of the horizontal\naxis.\n\nFor more information, see DetectText in the Amazon Rekognition Developer Guide.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/DetectText.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "GetCelebrityInfo": {
            "main": "./lib/actions/GetCelebrityInfo.js",
            "title": "GetCelebrityInfo",
            "description": "Gets the name and additional information about a celebrity based on his or her\nAmazon Rekognition ID. The additional information is returned as an array of\nURLs. If there is no additional information about the celebrity, this list is\nempty.\n\nFor more information, see Recognizing Celebrities in an Image in the Amazon\nRekognition Developer Guide.\n\nThis operation requires permissions to perform the rekognition:GetCelebrityInfo \naction.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/GetCelebrityInfo.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "GetCelebrityRecognition": {
            "main": "./lib/actions/GetCelebrityRecognition.js",
            "title": "GetCelebrityRecognition",
            "description": "Gets the celebrity recognition results for a Amazon Rekognition Video analysis\nstarted by StartCelebrityRecognition.\n\nCelebrity recognition in a video is an asynchronous operation. Analysis is\nstarted by a call to StartCelebrityRecognition which returns a job identifier (\nJobId). When the celebrity recognition operation finishes, Amazon Rekognition\nVideo publishes a completion status to the Amazon Simple Notification Service\ntopic registered in the initial call to StartCelebrityRecognition. To get the\nresults of the celebrity recognition analysis, first check that the status value\npublished to the Amazon SNS topic is SUCCEEDED. If so, call \nGetCelebrityDetection and pass the job identifier (JobId) from the initial call\nto StartCelebrityDetection. \n\nFor more information, see Working With Stored Videos in the Amazon Rekognition\nDeveloper Guide.\n\n GetCelebrityRecognition returns detected celebrities and the time(s) they are\ndetected in an array (Celebrities) of CelebrityRecognition objects. Each \nCelebrityRecognition contains information about the celebrity in a \nCelebrityDetail object and the time, Timestamp, the celebrity was detected. \n\n GetCelebrityRecognition only returns the default facial attributes (BoundingBox\n, Confidence, Landmarks, Pose, and Quality). The other facial attributes listed\nin the Face object of the following response syntax are not returned. For more\ninformation, see FaceDetail in the Amazon Rekognition Developer Guide. \n\nBy default, the Celebrities array is sorted by time (milliseconds from the start\nof the video). You can also sort the array by celebrity by specifying the value \nID in the SortBy input parameter.\n\nThe CelebrityDetail object includes the celebrity identifer and additional\ninformation urls. If you don't store the additional information urls, you can\nget them later by calling GetCelebrityInfo with the celebrity identifer.\n\nNo information is returned for faces not recognized as celebrities.\n\nUse MaxResults parameter to limit the number of labels returned. If there are\nmore results than specified in MaxResults, the value of NextToken in the\noperation response contains a pagination token for getting the next set of\nresults. To get the next page of results, call GetCelebrityDetection and\npopulate the NextToken request parameter with the token value returned from the\nprevious call to GetCelebrityRecognition.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/GetCelebrityRecognition.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "GetContentModeration": {
            "main": "./lib/actions/GetContentModeration.js",
            "title": "GetContentModeration",
            "description": "Gets the content moderation analysis results for a Amazon Rekognition Video\nanalysis started by StartContentModeration.\n\nContent moderation analysis of a video is an asynchronous operation. You start\nanalysis by calling StartContentModeration which returns a job identifier (JobId\n). When analysis finishes, Amazon Rekognition Video publishes a completion\nstatus to the Amazon Simple Notification Service topic registered in the initial\ncall to StartContentModeration. To get the results of the content moderation\nanalysis, first check that the status value published to the Amazon SNS topic is \nSUCCEEDED. If so, call GetContentModeration and pass the job identifier (JobId)\nfrom the initial call to StartContentModeration. \n\nFor more information, see Working with Stored Videos in the Amazon Rekognition\nDevlopers Guide.\n\n GetContentModeration returns detected content moderation labels, and the time\nthey are detected, in an array, ModerationLabels, of ContentModerationDetection \nobjects. \n\nBy default, the moderated labels are returned sorted by time, in milliseconds\nfrom the start of the video. You can also sort them by moderated label by\nspecifying NAME for the SortBy input parameter. \n\nSince video analysis can return a large number of results, use the MaxResults \nparameter to limit the number of labels returned in a single call to \nGetContentModeration. If there are more results than specified in MaxResults,\nthe value of NextToken in the operation response contains a pagination token for\ngetting the next set of results. To get the next page of results, call \nGetContentModeration and populate the NextToken request parameter with the value\nof NextToken returned from the previous call to GetContentModeration.\n\nFor more information, see Detecting Unsafe Content in the Amazon Rekognition\nDeveloper Guide.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/GetContentModeration.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "GetFaceDetection": {
            "main": "./lib/actions/GetFaceDetection.js",
            "title": "GetFaceDetection",
            "description": "Gets face detection results for a Amazon Rekognition Video analysis started by \nStartFaceDetection.\n\nFace detection with Amazon Rekognition Video is an asynchronous operation. You\nstart face detection by calling StartFaceDetection which returns a job\nidentifier (JobId). When the face detection operation finishes, Amazon\nRekognition Video publishes a completion status to the Amazon Simple\nNotification Service topic registered in the initial call to StartFaceDetection.\nTo get the results of the face detection operation, first check that the status\nvalue published to the Amazon SNS topic is SUCCEEDED. If so, call \nGetFaceDetection and pass the job identifier (JobId) from the initial call to \nStartFaceDetection.\n\n GetFaceDetection returns an array of detected faces (Faces) sorted by the time\nthe faces were detected. \n\nUse MaxResults parameter to limit the number of labels returned. If there are\nmore results than specified in MaxResults, the value of NextToken in the\noperation response contains a pagination token for getting the next set of\nresults. To get the next page of results, call GetFaceDetection and populate the \nNextToken request parameter with the token value returned from the previous call\nto GetFaceDetection.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/GetFaceDetection.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "GetFaceSearch": {
            "main": "./lib/actions/GetFaceSearch.js",
            "title": "GetFaceSearch",
            "description": "Gets the face search results for Amazon Rekognition Video face search started by \nStartFaceSearch. The search returns faces in a collection that match the faces\nof persons detected in a video. It also includes the time(s) that faces are\nmatched in the video.\n\nFace search in a video is an asynchronous operation. You start face search by\ncalling to StartFaceSearch which returns a job identifier (JobId). When the\nsearch operation finishes, Amazon Rekognition Video publishes a completion\nstatus to the Amazon Simple Notification Service topic registered in the initial\ncall to StartFaceSearch. To get the search results, first check that the status\nvalue published to the Amazon SNS topic is SUCCEEDED. If so, call GetFaceSearch \nand pass the job identifier (JobId) from the initial call to StartFaceSearch.\n\nFor more information, see Searching Faces in a Collection in the Amazon\nRekognition Developer Guide.\n\nThe search results are retured in an array, Persons, of PersonMatch objects.\nEachPersonMatch element contains details about the matching faces in the input\ncollection, person information (facial attributes, bounding boxes, and person\nidentifer) for the matched person, and the time the person was matched in the\nvideo.\n\n GetFaceSearch only returns the default facial attributes (BoundingBox, \nConfidence, Landmarks, Pose, and Quality). The other facial attributes listed in\nthe Face object of the following response syntax are not returned. For more\ninformation, see FaceDetail in the Amazon Rekognition Developer Guide. \n\nBy default, the Persons array is sorted by the time, in milliseconds from the\nstart of the video, persons are matched. You can also sort by persons by\nspecifying INDEX for the SORTBY input parameter.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/GetFaceSearch.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "GetLabelDetection": {
            "main": "./lib/actions/GetLabelDetection.js",
            "title": "GetLabelDetection",
            "description": "Gets the label detection results of a Amazon Rekognition Video analysis started\nby StartLabelDetection. \n\nThe label detection operation is started by a call to StartLabelDetection which\nreturns a job identifier (JobId). When the label detection operation finishes,\nAmazon Rekognition publishes a completion status to the Amazon Simple\nNotification Service topic registered in the initial call to StartlabelDetection\n. To get the results of the label detection operation, first check that the\nstatus value published to the Amazon SNS topic is SUCCEEDED. If so, call \nGetLabelDetection and pass the job identifier (JobId) from the initial call to \nStartLabelDetection.\n\n GetLabelDetection returns an array of detected labels (Labels) sorted by the\ntime the labels were detected. You can also sort by the label name by specifying \nNAME for the SortBy input parameter.\n\nThe labels returned include the label name, the percentage confidence in the\naccuracy of the detected label, and the time the label was detected in the\nvideo.\n\nThe returned labels also include bounding box information for common objects, a\nhierarchical taxonomy of detected labels, and the version of the label model\nused for detection.\n\nUse MaxResults parameter to limit the number of labels returned. If there are\nmore results than specified in MaxResults, the value of NextToken in the\noperation response contains a pagination token for getting the next set of\nresults. To get the next page of results, call GetlabelDetection and populate\nthe NextToken request parameter with the token value returned from the previous\ncall to GetLabelDetection.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/GetLabelDetection.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "GetPersonTracking": {
            "main": "./lib/actions/GetPersonTracking.js",
            "title": "GetPersonTracking",
            "description": "Gets the path tracking results of a Amazon Rekognition Video analysis started by \nStartPersonTracking.\n\nThe person path tracking operation is started by a call to StartPersonTracking \nwhich returns a job identifier (JobId). When the operation finishes, Amazon\nRekognition Video publishes a completion status to the Amazon Simple\nNotification Service topic registered in the initial call to StartPersonTracking\n.\n\nTo get the results of the person path tracking operation, first check that the\nstatus value published to the Amazon SNS topic is SUCCEEDED. If so, call \nGetPersonTracking and pass the job identifier (JobId) from the initial call to \nStartPersonTracking.\n\n GetPersonTracking returns an array, Persons, of tracked persons and the time(s)\ntheir paths were tracked in the video. \n\n GetPersonTracking only returns the default facial attributes (BoundingBox, \nConfidence, Landmarks, Pose, and Quality). The other facial attributes listed in\nthe Face object of the following response syntax are not returned. \n\nFor more information, see FaceDetail in the Amazon Rekognition Developer Guide.\n\nBy default, the array is sorted by the time(s) a person's path is tracked in the\nvideo. You can sort by tracked persons by specifying INDEX for the SortBy input\nparameter.\n\nUse the MaxResults parameter to limit the number of items returned. If there are\nmore results than specified in MaxResults, the value of NextToken in the\noperation response contains a pagination token for getting the next set of\nresults. To get the next page of results, call GetPersonTracking and populate\nthe NextToken request parameter with the token value returned from the previous\ncall to GetPersonTracking.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/GetPersonTracking.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "IndexFaces": {
            "main": "./lib/actions/IndexFaces.js",
            "title": "IndexFaces",
            "description": "Detects faces in the input image and adds them to the specified collection. \n\nAmazon Rekognition doesn't save the actual faces that are detected. Instead, the\nunderlying detection algorithm first detects the faces in the input image. For\neach face, the algorithm extracts facial features into a feature vector, and\nstores it in the backend database. Amazon Rekognition uses feature vectors when\nit performs face match and search operations using the SearchFaces and \nSearchFacesByImage operations.\n\nFor more information, see Adding Faces to a Collection in the Amazon Rekognition\nDeveloper Guide.\n\nTo get the number of faces in a collection, call DescribeCollection. \n\nIf you're using version 1.0 of the face detection model, IndexFaces indexes the\n15 largest faces in the input image. Later versions of the face detection model\nindex the 100 largest faces in the input image. \n\nIf you're using version 4 or later of the face model, image orientation\ninformation is not returned in the OrientationCorrection field. \n\nTo determine which version of the model you're using, call DescribeCollection \nand supply the collection ID. You can also get the model version from the value\nof FaceModelVersion in the response from IndexFaces \n\nFor more information, see Model Versioning in the Amazon Rekognition Developer\nGuide.\n\nIf you provide the optional ExternalImageID for the input image you provided,\nAmazon Rekognition associates this ID with all faces that it detects. When you\ncall the ListFaces operation, the response returns the external ID. You can use\nthis external image ID to create a client-side index to associate the faces with\neach image. You can then use the index to find all faces in an image.\n\nYou can specify the maximum number of faces to index with the MaxFaces input\nparameter. This is useful when you want to index the largest faces in an image\nand don't want to index smaller faces, such as those belonging to people\nstanding in the background.\n\nThe QualityFilter input parameter allows you to filter out detected faces that\ndon't meet the required quality bar chosen by Amazon Rekognition. The quality\nbar is based on a variety of common use cases. By default, IndexFaces filters\ndetected faces. You can also explicitly filter detected faces by specifying AUTO \nfor the value of QualityFilter. If you do not want to filter detected faces,\nspecify NONE. \n\nTo use quality filtering, you need a collection associated with version 3 of the\nface model. To get the version of the face model associated with a collection,\ncall DescribeCollection. \n\nInformation about faces detected in an image, but not indexed, is returned in an\narray of UnindexedFace objects, UnindexedFaces. Faces aren't indexed for reasons\nsuch as:\n\n *  The number of faces detected exceeds the value of the MaxFaces request\n   parameter.\n   \n   \n *  The face is too small compared to the image dimensions.\n   \n   \n *  The face is too blurry.\n   \n   \n *  The image is too dark.\n   \n   \n *  The face has an extreme pose.\n   \n   \n\nIn response, the IndexFaces operation returns an array of metadata for all\ndetected faces, FaceRecords. This includes: \n\n *  The bounding box, BoundingBox, of the detected face. \n   \n   \n *  A confidence value, Confidence, which indicates the confidence that the\n   bounding box contains a face.\n   \n   \n *  A face ID, FaceId, assigned by the service for each face that's detected and\n   stored.\n   \n   \n *  An image ID, ImageId, assigned by the service for the input image.\n   \n   \n\nIf you request all facial attributes (by using the detectionAttributes \nparameter), Amazon Rekognition returns detailed facial attributes, such as\nfacial landmarks (for example, location of eye and mouth) and other facial\nattributes like gender. If you provide the same image, specify the same\ncollection, and use the same external ID in the IndexFaces operation, Amazon\nRekognition doesn't save duplicate face metadata.\n\n \n\nThe input image is passed either as base64-encoded image bytes, or as a\nreference to an image in an Amazon S3 bucket. If you use the AWS CLI to call\nAmazon Rekognition operations, passing image bytes isn't supported. The image\nmust be formatted as a PNG or JPEG file. \n\nThis operation requires permissions to perform the rekognition:IndexFaces \naction.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/IndexFaces.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "ListCollections": {
            "main": "./lib/actions/ListCollections.js",
            "title": "ListCollections",
            "description": "Returns list of collection IDs in your account. If the result is truncated, the\nresponse also provides a NextToken that you can use in the subsequent request to\nfetch the next set of collection IDs.\n\nFor an example, see Listing Collections in the Amazon Rekognition Developer\nGuide.\n\nThis operation requires permissions to perform the rekognition:ListCollections \naction.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/ListCollections.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "ListFaces": {
            "main": "./lib/actions/ListFaces.js",
            "title": "ListFaces",
            "description": "Returns metadata for faces in the specified collection. This metadata includes\ninformation such as the bounding box coordinates, the confidence (that the\nbounding box contains a face), and face ID. For an example, see Listing Faces in\na Collection in the Amazon Rekognition Developer Guide.\n\nThis operation requires permissions to perform the rekognition:ListFaces action.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/ListFaces.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "ListStreamProcessors": {
            "main": "./lib/actions/ListStreamProcessors.js",
            "title": "ListStreamProcessors",
            "description": "Gets a list of stream processors that you have created with <a>CreateStreamProcessor</a>. ",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/ListStreamProcessors.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "RecognizeCelebrities": {
            "main": "./lib/actions/RecognizeCelebrities.js",
            "title": "RecognizeCelebrities",
            "description": "Returns an array of celebrities recognized in the input image. For more\ninformation, see Recognizing Celebrities in the Amazon Rekognition Developer\nGuide. \n\n RecognizeCelebrities returns the 100 largest faces in the image. It lists\nrecognized celebrities in the CelebrityFaces array and unrecognized faces in the \nUnrecognizedFaces array. RecognizeCelebrities doesn't return celebrities whose\nfaces aren't among the largest 100 faces in the image.\n\nFor each celebrity recognized, RecognizeCelebrities returns a Celebrity object.\nThe Celebrity object contains the celebrity name, ID, URL links to additional\ninformation, match confidence, and a ComparedFace object that you can use to\nlocate the celebrity's face on the image.\n\nAmazon Rekognition doesn't retain information about which images a celebrity has\nbeen recognized in. Your application must store this information and use the \nCelebrity ID property as a unique identifier for the celebrity. If you don't\nstore the celebrity name or additional information URLs returned by \nRecognizeCelebrities, you will need the ID to identify the celebrity in a call\nto the GetCelebrityInfo operation.\n\nYou pass the input image either as base64-encoded image bytes or as a reference\nto an image in an Amazon S3 bucket. If you use the AWS CLI to call Amazon\nRekognition operations, passing image bytes is not supported. The image must be\neither a PNG or JPEG formatted file. \n\nFor an example, see Recognizing Celebrities in an Image in the Amazon\nRekognition Developer Guide.\n\nThis operation requires permissions to perform the \nrekognition:RecognizeCelebrities operation.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/RecognizeCelebrities.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "SearchFaces": {
            "main": "./lib/actions/SearchFaces.js",
            "title": "SearchFaces",
            "description": "For a given input face ID, searches for matching faces in the collection the\nface belongs to. You get a face ID when you add a face to the collection using\nthe IndexFaces operation. The operation compares the features of the input face\nwith faces in the specified collection. \n\nYou can also search faces without indexing faces by using the SearchFacesByImage \noperation.\n\n The operation response returns an array of faces that match, ordered by\nsimilarity score with the highest similarity first. More specifically, it is an\narray of metadata for each face match that is found. Along with the metadata,\nthe response also includes a confidence value for each face match, indicating\nthe confidence that the specific face matches the input face. \n\nFor an example, see Searching for a Face Using Its Face ID in the Amazon\nRekognition Developer Guide.\n\nThis operation requires permissions to perform the rekognition:SearchFaces \naction.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/SearchFaces.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "SearchFacesByImage": {
            "main": "./lib/actions/SearchFacesByImage.js",
            "title": "SearchFacesByImage",
            "description": "For a given input image, first detects the largest face in the image, and then\nsearches the specified collection for matching faces. The operation compares the\nfeatures of the input face with faces in the specified collection. \n\nTo search for all faces in an input image, you might first call the IndexFaces \noperation, and then use the face IDs returned in subsequent calls to the \nSearchFaces operation. \n\n You can also call the DetectFaces operation and use the bounding boxes in the\nresponse to make face crops, which then you can pass in to the \nSearchFacesByImage operation. \n\nYou pass the input image either as base64-encoded image bytes or as a reference\nto an image in an Amazon S3 bucket. If you use the AWS CLI to call Amazon\nRekognition operations, passing image bytes is not supported. The image must be\neither a PNG or JPEG formatted file. \n\n The response returns an array of faces that match, ordered by similarity score\nwith the highest similarity first. More specifically, it is an array of metadata\nfor each face match found. Along with the metadata, the response also includes a \nsimilarity indicating how similar the face is to the input face. In the\nresponse, the operation also returns the bounding box (and a confidence level\nthat the bounding box contains a face) of the face that Amazon Rekognition used\nfor the input image. \n\nFor an example, Searching for a Face Using an Image in the Amazon Rekognition\nDeveloper Guide.\n\nThis operation requires permissions to perform the \nrekognition:SearchFacesByImage action.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/SearchFacesByImage.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "StartCelebrityRecognition": {
            "main": "./lib/actions/StartCelebrityRecognition.js",
            "title": "StartCelebrityRecognition",
            "description": "Starts asynchronous recognition of celebrities in a stored video.\n\nAmazon Rekognition Video can detect celebrities in a video must be stored in an\nAmazon S3 bucket. Use Video to specify the bucket name and the filename of the\nvideo. StartCelebrityRecognition returns a job identifier (JobId) which you use\nto get the results of the analysis. When celebrity recognition analysis is\nfinished, Amazon Rekognition Video publishes a completion status to the Amazon\nSimple Notification Service topic that you specify in NotificationChannel. To\nget the results of the celebrity recognition analysis, first check that the\nstatus value published to the Amazon SNS topic is SUCCEEDED. If so, call \nGetCelebrityRecognition and pass the job identifier (JobId) from the initial\ncall to StartCelebrityRecognition. \n\nFor more information, see Recognizing Celebrities in the Amazon Rekognition\nDeveloper Guide.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/StartCelebrityRecognition.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "StartContentModeration": {
            "main": "./lib/actions/StartContentModeration.js",
            "title": "StartContentModeration",
            "description": " Starts asynchronous detection of explicit or suggestive adult content in a\nstored video.\n\nAmazon Rekognition Video can moderate content in a video stored in an Amazon S3\nbucket. Use Video to specify the bucket name and the filename of the video. \nStartContentModeration returns a job identifier (JobId) which you use to get the\nresults of the analysis. When content moderation analysis is finished, Amazon\nRekognition Video publishes a completion status to the Amazon Simple\nNotification Service topic that you specify in NotificationChannel.\n\nTo get the results of the content moderation analysis, first check that the\nstatus value published to the Amazon SNS topic is SUCCEEDED. If so, call \nGetContentModeration and pass the job identifier (JobId) from the initial call\nto StartContentModeration. \n\nFor more information, see Detecting Unsafe Content in the Amazon Rekognition\nDeveloper Guide.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/StartContentModeration.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "StartFaceDetection": {
            "main": "./lib/actions/StartFaceDetection.js",
            "title": "StartFaceDetection",
            "description": "Starts asynchronous detection of faces in a stored video.\n\nAmazon Rekognition Video can detect faces in a video stored in an Amazon S3\nbucket. Use Video to specify the bucket name and the filename of the video. \nStartFaceDetection returns a job identifier (JobId) that you use to get the\nresults of the operation. When face detection is finished, Amazon Rekognition\nVideo publishes a completion status to the Amazon Simple Notification Service\ntopic that you specify in NotificationChannel. To get the results of the face\ndetection operation, first check that the status value published to the Amazon\nSNS topic is SUCCEEDED. If so, call GetFaceDetection and pass the job identifier\n(JobId) from the initial call to StartFaceDetection.\n\nFor more information, see Detecting Faces in a Stored Video in the Amazon\nRekognition Developer Guide.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/StartFaceDetection.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "StartFaceSearch": {
            "main": "./lib/actions/StartFaceSearch.js",
            "title": "StartFaceSearch",
            "description": "Starts the asynchronous search for faces in a collection that match the faces of\npersons detected in a stored video.\n\nThe video must be stored in an Amazon S3 bucket. Use Video to specify the bucket\nname and the filename of the video. StartFaceSearch returns a job identifier (\nJobId) which you use to get the search results once the search has completed.\nWhen searching is finished, Amazon Rekognition Video publishes a completion\nstatus to the Amazon Simple Notification Service topic that you specify in \nNotificationChannel. To get the search results, first check that the status\nvalue published to the Amazon SNS topic is SUCCEEDED. If so, call GetFaceSearch \nand pass the job identifier (JobId) from the initial call to StartFaceSearch.\nFor more information, see procedure-person-search-videos.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/StartFaceSearch.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "StartLabelDetection": {
            "main": "./lib/actions/StartLabelDetection.js",
            "title": "StartLabelDetection",
            "description": "Starts asynchronous detection of labels in a stored video.\n\nAmazon Rekognition Video can detect labels in a video. Labels are instances of\nreal-world entities. This includes objects like flower, tree, and table; events\nlike wedding, graduation, and birthday party; concepts like landscape, evening,\nand nature; and activities like a person getting out of a car or a person\nskiing.\n\nThe video must be stored in an Amazon S3 bucket. Use Video to specify the bucket\nname and the filename of the video. StartLabelDetection returns a job identifier\n(JobId) which you use to get the results of the operation. When label detection\nis finished, Amazon Rekognition Video publishes a completion status to the\nAmazon Simple Notification Service topic that you specify in NotificationChannel\n.\n\nTo get the results of the label detection operation, first check that the status\nvalue published to the Amazon SNS topic is SUCCEEDED. If so, call \nGetLabelDetection and pass the job identifier (JobId) from the initial call to \nStartLabelDetection.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/StartLabelDetection.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "StartPersonTracking": {
            "main": "./lib/actions/StartPersonTracking.js",
            "title": "StartPersonTracking",
            "description": "Starts the asynchronous tracking of a person's path in a stored video.\n\nAmazon Rekognition Video can track the path of people in a video stored in an\nAmazon S3 bucket. Use Video to specify the bucket name and the filename of the\nvideo. StartPersonTracking returns a job identifier (JobId) which you use to get\nthe results of the operation. When label detection is finished, Amazon\nRekognition publishes a completion status to the Amazon Simple Notification\nService topic that you specify in NotificationChannel. \n\nTo get the results of the person detection operation, first check that the\nstatus value published to the Amazon SNS topic is SUCCEEDED. If so, call \nGetPersonTracking and pass the job identifier (JobId) from the initial call to \nStartPersonTracking.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/StartPersonTracking.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "StartStreamProcessor": {
            "main": "./lib/actions/StartStreamProcessor.js",
            "title": "StartStreamProcessor",
            "description": "Starts processing a stream processor. You create a stream processor by calling <a>CreateStreamProcessor</a>. To tell <code>StartStreamProcessor</code> which stream processor to start, use the value of the <code>Name</code> field specified in the call to <code>CreateStreamProcessor</code>.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/StartStreamProcessor.in.json",
                "out": {
                    "type": "object"
                }
            }
        },
        "StopStreamProcessor": {
            "main": "./lib/actions/StopStreamProcessor.js",
            "title": "StopStreamProcessor",
            "description": "Stops a running stream processor that was created by <a>CreateStreamProcessor</a>.",
            "fields": {
                "verbose": {
                    "viewClass": "CheckBoxView",
                    "label": "Debug this step (log more data)"
                }
            },
            "metadata": {
                "in": "./lib/schemas/StopStreamProcessor.in.json",
                "out": {
                    "type": "object"
                }
            }
        }
    }
}